# The main job for dab.
resources:
  jobs:
    dab_job:
      name: dab_job
      trigger:
        # Run this job every day, exactly one day from the last run; see https://docs.databricks.com/api/workspace/jobs/create#trigger
        periodic:
          interval: 1
          unit: DAYS
      email_notifications:
        on_failure:
          - nguliyev@kinetechconsult.com
      tasks:
        - task_key: notebook_task
          job_cluster_key: job_cluster
          notebook_task:
            # remove hardcoding and use variable
            # notebook_path: ../src/notebook.ipynb
            notebook_path: ${var.dab_notebook}  # will use variable which defined within databricks.yml
            # setting a parameter from notebook_task
            base_parameters:
              notebook_param: "my param" # we can read this from notebook itself with dbutils.widgets.get

        - task_key: notebook_task_2
          job_cluster_key: job_cluster_2
          depends_on: # this task will be depend on above task
            - task_key: notebook_task  
          notebook_task:
            notebook_path: ${var.dab_notebook}
            # we dynamically get value that comes from hello.ipynb which executed in first above task.
            base_parameters:
              from_t1: "{{tasks.notebook_task.values.from_t1}}"  # we dynamically get value. {{task.<name_of_task>.values.<key of value we set in notebook>}}

        - task_key: notebook_task_3
          # for notebook task 3 we will use our existing cluster which we have in dbx and we specified variable in dab_clusters.yml
          # so our variable in dab_clusters.yml will lookup for the cluster id of our existing cluster then will paste that cluster_id to below.
          existing_cluster_id: ${var.existing_cluster}
          notebook_task:
            notebook_path: ${var.dab_notebook}

        - task_key: notebook_task_4
          job_cluster_key: job_cluster_4
          depends_on:
            - task_key: notebook_task_2 
          notebook_task:
            notebook_path: ../notebooks/hello4.ipynb # here we hardcode the notebook path because we use different notebook.

        - task_key: notebook_task_5
          job_cluster_key: job_cluster_4
          depends_on:
            - task_key: notebook_task_4
          notebook_task:
            notebook_path: ../notebooks/hello4.ipynb

      job_clusters:
        - job_cluster_key: job_cluster
          # we will use variable here for cluster configuration
          new_cluster: ${var.dab_cluster_task_1}
          # another variable
        - job_cluster_key: job_cluster_2
          new_cluster: ${var.dab_cluster_task_2}
        - job_cluster_key: job_cluster_4
          new_cluster: ${var.dab_cluster_task_4}
          




# Above is our just 1 job. we can create a new job in this YML file. but Databricks recommends 
# creation of each job in their own YML file. 