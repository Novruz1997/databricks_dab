# The main job for dab.
resources:
  jobs:
    dab_job:
      name: dab_job
      trigger:
        # Run this job every day, exactly one day from the last run; see https://docs.databricks.com/api/workspace/jobs/create#trigger
        periodic:
          interval: 1
          unit: DAYS
      email_notifications:
        on_failure:
          - nguliyev@kinetechconsult.com
      tasks:
        - task_key: notebook_task
          job_cluster_key: job_cluster
          notebook_task:
            # remove hardcoding and use variable
            # notebook_path: ../src/notebook.ipynb
            notebook_path: ${var.dab_notebook}  # will use variable which defined within databricks.yml

        - task_key: notebook_task_2
          job_cluster_key: job_cluster_2
          notebook_task:
            notebook_path: ${var.dab_notebook}
        - task_key: notebook_task_3
          # for notebook task 3 we will use our existing cluster which we have in dbx and we specified variable in dab_clusters.yml
          # so our variable in dab_clusters.yml will lookup for the cluster id of our existing cluster then will paste that cluster_id to below.
          existing_cluster_id: ${var.existing_cluster}
          notebook_task:
            notebook_path: ${var.dab_notebook}
      job_clusters:
        - job_cluster_key: job_cluster
          # we will use variable here for cluster configuration
          new_cluster: ${var.dab_cluster_task_1}
          # another variable
        - job_cluster_key: job_cluster_2
          new_cluster: ${var.dab_cluster_task_2}
          




# Above is our just 1 job. we can create a new job in this YML file. but Databricks recommends 
# creation of each job in their own YML file. 