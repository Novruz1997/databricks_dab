# this workflow validates, deploys and runs the specified bundle
# within the pre-production target name "DEV"
name: "DEV deployment"

# ensures that only a single job or workflow using the same concurrency group
# runs at a time
concurrency: "1"

# Trigger the worflow whenever a pull request is opened against the repo's main branch
# or an existing pull request's head branch is updated
# on: [push]

on: 
  pull_request:
    branches: [main]

jobs:
  # used by the "pipeline_update" job to deploy the bundle
  # Bundle validation is automatically performed as part of this deployment
  # if walidation fails the workflow fails
  deploy:
    name: "Deploy bundle"
    runs-on: ubuntu-latest
    environment: dev

    steps:
      # Check out this repo, so that this workflow can access it
      - uses: actions/checkout@v3

      # Download the databricks CLI.
      - uses: databricks/setup-cli@main

      # deploy the bundle to the "dev" target as defined
      # in the bundle's settings file
      - run: databricks bundle deploy
        working-directory: dab
        env:
          DATABRICKS_TOKEN: ${{ secrets.SP_TOKEN }}
          DATABRICKS_BUNDLE_ENV: dev # this is equal to -t dev in line 31.

  # validate, deploy, and then run bundle
  pipeline_update:
    name: "Run pipeline update"
    runs-on: ubuntu-latest
    environment: dev


    # run the "deploy" (above) job first
    needs:
      - deploy

    steps:
      # check out this repo so that this workflow can access it
      - uses: actions/checkout@v3

      # Use the downloaded databricks CLI
      - uses: databricks/setup-cli@main

      # run the databricks workflow named "dab_job" as defined in the 
      # bundle that just was deployed
      - run: databricks bundle run dab_job --refresh-all
        working-directory: dab
        env:
          DATABRICKS_TOKEN: ${{ secrets.SP_TOKEN }}
          DATABRICKS_BUNDLE_ENV: dev

